{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available.Training on GPU\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print(\"Cuda is not available.Training on CPU..\")\n",
    "else:\n",
    "    print(\"Cuda is available.Training on GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Dataloader(is_train = True,batch_size = 20):\n",
    "    if is_train:\n",
    "        transform_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                              transforms.RandomRotation(10),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "        transform_validation = transforms.Compose([transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "        train_data = datasets.CIFAR100('Data',train = True, download = True,transform = transform_train)\n",
    "        size = len(train_data)\n",
    "        indices = list(range(size))\n",
    "        np.random.shuffle(indices)\n",
    "        split =  int(np.floor(0.2 * size))\n",
    "        train_idx,val_idx = indices[split:], indices[:split]\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, sampler = train_sampler, num_workers = 0)\n",
    "        val_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, sampler = val_sampler, num_workers = 0)\n",
    "        return train_loader, val_loader\n",
    "    else:\n",
    "        transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "        test_data = datasets.CIFAR100('Data',train = False, download = False,transform = transform_test)\n",
    "        test_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size, num_workers = 0)\n",
    "    \n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "2000 500 500\n"
     ]
    }
   ],
   "source": [
    "train_loader,val_loader = get_Dataloader()\n",
    "test_loader = get_Dataloader(is_train = False)\n",
    "print(len(train_loader),len(val_loader),len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "#                                      transforms.RandomRotation(10),\n",
    "#                                      transforms.ToTensor(),\n",
    "#                                      transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "# transform_validation = transforms.Compose([transforms.ToTensor(),\n",
    "#                                           transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "# train_data = datasets.CIFAR10('data', train = True, download = True, transform = transform_train)\n",
    "# test_data = datasets.CIFAR10('data',train = False,download = True, transform = transform_validation)\n",
    "# batch_size = 20\n",
    "# num_worker = 0\n",
    "# num_train = len(train_data)\n",
    "# indices = list(range(num_train))\n",
    "# np.random.shuffle(indices)\n",
    "# split =  int(np.floor(0.2 * num_train))\n",
    "# train_idx,val_idx = indices[split:], indices[:split]\n",
    "\n",
    "# train_sampler = SubsetRandomSampler(train_idx)\n",
    "# val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = train_sampler, num_workers = num_worker )\n",
    "# valloader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = val_sampler, num_workers = num_worker )\n",
    "# testloader = torch.utils.data.DataLoader(test_data, batch_size = batch_size,num_workers = num_worker)\n",
    "# classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "#            'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "    \n",
    "    layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image):\n",
    "    image = image/2 + 0.5 #un-normalizing\n",
    "    plt.imshow(np.transpose(image,(1,2,0)))\n",
    "\n",
    "classes = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
    "    'worm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=100, bias=True)\n",
      "  (drop): Dropout(p=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        \n",
    "        self.conv1 = conv(3,64,3)\n",
    "        self.conv2 = conv(64,128,3)\n",
    "        self.conv3 = conv(128,256,3)\n",
    "        self.conv4 = conv(256,512,3)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512*2*2,1024)\n",
    "        self.fc2 = nn.Linear(1024,512)\n",
    "        self.fc3 = nn.Linear(512,100)\n",
    "        self.drop = nn.Dropout(.2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.pool(F.elu(self.conv1(x)))\n",
    "        x = self.pool(F.elu(self.conv2(x)))\n",
    "        x = self.pool(F.elu(self.conv3(x)))\n",
    "        x = self.pool(F.elu(self.conv4(x)))\n",
    "        x = x.view(-1,512*2*2)\n",
    "        x = self.drop(x)\n",
    "        x = self.drop(self.fc1(x))\n",
    "        \n",
    "        x = self.drop(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "network = Network()\n",
    "print(network)\n",
    "        \n",
    "if train_on_gpu:\n",
    "    network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(network.parameters(),lr= 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Execution : Mon Sep 16 06:00:05 2019\n",
      "Epoch : 1 \tTraining Loss :  4.2603 \tValidation Loss :  3.9431\n",
      "Validation Loss decreased ( inf --->  3.9431. Saving Model)\n",
      "Epoch : 2 \tTraining Loss :  3.7883 \tValidation Loss :  3.5616\n",
      "Validation Loss decreased ( 3.9431 --->  3.5616. Saving Model)\n",
      "Epoch : 3 \tTraining Loss :  3.4945 \tValidation Loss :  3.3110\n",
      "Validation Loss decreased ( 3.5616 --->  3.3110. Saving Model)\n",
      "Epoch : 4 \tTraining Loss :  3.2714 \tValidation Loss :  3.1171\n",
      "Validation Loss decreased ( 3.3110 --->  3.1171. Saving Model)\n",
      "Epoch : 5 \tTraining Loss :  3.0849 \tValidation Loss :  2.9247\n",
      "Validation Loss decreased ( 3.1171 --->  2.9247. Saving Model)\n",
      "Epoch : 6 \tTraining Loss :  2.9366 \tValidation Loss :  2.8084\n",
      "Validation Loss decreased ( 2.9247 --->  2.8084. Saving Model)\n",
      "Epoch : 7 \tTraining Loss :  2.8092 \tValidation Loss :  2.7324\n",
      "Validation Loss decreased ( 2.8084 --->  2.7324. Saving Model)\n",
      "Epoch : 8 \tTraining Loss :  2.7111 \tValidation Loss :  2.6360\n",
      "Validation Loss decreased ( 2.7324 --->  2.6360. Saving Model)\n",
      "Epoch : 9 \tTraining Loss :  2.6265 \tValidation Loss :  2.5594\n",
      "Validation Loss decreased ( 2.6360 --->  2.5594. Saving Model)\n",
      "Epoch : 10 \tTraining Loss :  2.5499 \tValidation Loss :  2.5006\n",
      "Validation Loss decreased ( 2.5594 --->  2.5006. Saving Model)\n",
      "Epoch : 11 \tTraining Loss :  2.4861 \tValidation Loss :  2.4332\n",
      "Validation Loss decreased ( 2.5006 --->  2.4332. Saving Model)\n",
      "Epoch : 12 \tTraining Loss :  2.4277 \tValidation Loss :  2.4140\n",
      "Validation Loss decreased ( 2.4332 --->  2.4140. Saving Model)\n",
      "Epoch : 13 \tTraining Loss :  2.3718 \tValidation Loss :  2.3190\n",
      "Validation Loss decreased ( 2.4140 --->  2.3190. Saving Model)\n",
      "Epoch : 14 \tTraining Loss :  2.3213 \tValidation Loss :  2.2925\n",
      "Validation Loss decreased ( 2.3190 --->  2.2925. Saving Model)\n",
      "Epoch : 15 \tTraining Loss :  2.2790 \tValidation Loss :  2.3191\n",
      "Epoch : 16 \tTraining Loss :  2.2247 \tValidation Loss :  2.2540\n",
      "Validation Loss decreased ( 2.2925 --->  2.2540. Saving Model)\n",
      "Epoch : 17 \tTraining Loss :  2.1867 \tValidation Loss :  2.1841\n",
      "Validation Loss decreased ( 2.2540 --->  2.1841. Saving Model)\n",
      "Epoch : 18 \tTraining Loss :  2.1476 \tValidation Loss :  2.1932\n",
      "Epoch : 19 \tTraining Loss :  2.1160 \tValidation Loss :  2.1400\n",
      "Validation Loss decreased ( 2.1841 --->  2.1400. Saving Model)\n",
      "Epoch : 20 \tTraining Loss :  2.0756 \tValidation Loss :  2.1272\n",
      "Validation Loss decreased ( 2.1400 --->  2.1272. Saving Model)\n",
      "Epoch : 21 \tTraining Loss :  2.0437 \tValidation Loss :  2.0958\n",
      "Validation Loss decreased ( 2.1272 --->  2.0958. Saving Model)\n",
      "Epoch : 22 \tTraining Loss :  2.0153 \tValidation Loss :  2.1143\n",
      "Epoch : 23 \tTraining Loss :  1.9806 \tValidation Loss :  2.0460\n",
      "Validation Loss decreased ( 2.0958 --->  2.0460. Saving Model)\n",
      "Epoch : 24 \tTraining Loss :  1.9492 \tValidation Loss :  2.0694\n",
      "Epoch : 25 \tTraining Loss :  1.9261 \tValidation Loss :  2.0122\n",
      "Validation Loss decreased ( 2.0460 --->  2.0122. Saving Model)\n",
      "Epoch : 26 \tTraining Loss :  1.9003 \tValidation Loss :  2.0158\n",
      "Epoch : 27 \tTraining Loss :  1.8724 \tValidation Loss :  1.9884\n",
      "Validation Loss decreased ( 2.0122 --->  1.9884. Saving Model)\n",
      "Epoch : 28 \tTraining Loss :  1.8544 \tValidation Loss :  1.9820\n",
      "Validation Loss decreased ( 1.9884 --->  1.9820. Saving Model)\n",
      "Epoch : 29 \tTraining Loss :  1.8195 \tValidation Loss :  1.9793\n",
      "Validation Loss decreased ( 1.9820 --->  1.9793. Saving Model)\n",
      "Epoch : 30 \tTraining Loss :  1.7964 \tValidation Loss :  1.9855\n",
      "Epoch : 31 \tTraining Loss :  1.7825 \tValidation Loss :  1.9362\n",
      "Validation Loss decreased ( 1.9793 --->  1.9362. Saving Model)\n",
      "Epoch : 32 \tTraining Loss :  1.7481 \tValidation Loss :  1.9469\n",
      "Epoch : 33 \tTraining Loss :  1.7329 \tValidation Loss :  1.9056\n",
      "Validation Loss decreased ( 1.9362 --->  1.9056. Saving Model)\n",
      "Epoch : 34 \tTraining Loss :  1.7083 \tValidation Loss :  1.9492\n",
      "Epoch : 35 \tTraining Loss :  1.6935 \tValidation Loss :  1.9042\n",
      "Validation Loss decreased ( 1.9056 --->  1.9042. Saving Model)\n",
      "Epoch : 36 \tTraining Loss :  1.6640 \tValidation Loss :  1.9617\n",
      "Epoch : 37 \tTraining Loss :  1.6478 \tValidation Loss :  1.8894\n",
      "Validation Loss decreased ( 1.9042 --->  1.8894. Saving Model)\n",
      "Epoch : 38 \tTraining Loss :  1.6202 \tValidation Loss :  1.9219\n",
      "Epoch : 39 \tTraining Loss :  1.6100 \tValidation Loss :  1.9113\n",
      "Epoch : 40 \tTraining Loss :  1.5875 \tValidation Loss :  1.8771\n",
      "Validation Loss decreased ( 1.8894 --->  1.8771. Saving Model)\n",
      "Epoch : 41 \tTraining Loss :  1.5724 \tValidation Loss :  1.8696\n",
      "Validation Loss decreased ( 1.8771 --->  1.8696. Saving Model)\n",
      "Epoch : 42 \tTraining Loss :  1.5521 \tValidation Loss :  1.8838\n",
      "Epoch : 43 \tTraining Loss :  1.5347 \tValidation Loss :  1.8532\n",
      "Validation Loss decreased ( 1.8696 --->  1.8532. Saving Model)\n",
      "Epoch : 44 \tTraining Loss :  1.5210 \tValidation Loss :  1.8706\n",
      "Epoch : 45 \tTraining Loss :  1.4944 \tValidation Loss :  1.8754\n",
      "Epoch : 46 \tTraining Loss :  1.4851 \tValidation Loss :  1.8560\n",
      "Epoch : 47 \tTraining Loss :  1.4695 \tValidation Loss :  1.8411\n",
      "Validation Loss decreased ( 1.8532 --->  1.8411. Saving Model)\n",
      "Epoch : 48 \tTraining Loss :  1.4516 \tValidation Loss :  1.8388\n",
      "Validation Loss decreased ( 1.8411 --->  1.8388. Saving Model)\n",
      "Epoch : 49 \tTraining Loss :  1.4353 \tValidation Loss :  1.8722\n",
      "Epoch : 50 \tTraining Loss :  1.4246 \tValidation Loss :  1.8129\n",
      "Validation Loss decreased ( 1.8388 --->  1.8129. Saving Model)\n",
      "Epoch : 51 \tTraining Loss :  1.4014 \tValidation Loss :  1.8407\n",
      "Epoch : 52 \tTraining Loss :  1.3849 \tValidation Loss :  1.8126\n",
      "Validation Loss decreased ( 1.8129 --->  1.8126. Saving Model)\n",
      "Epoch : 53 \tTraining Loss :  1.3656 \tValidation Loss :  1.8218\n",
      "Epoch : 54 \tTraining Loss :  1.3535 \tValidation Loss :  1.8460\n",
      "Epoch : 55 \tTraining Loss :  1.3378 \tValidation Loss :  1.8313\n",
      "Epoch : 56 \tTraining Loss :  1.3216 \tValidation Loss :  1.8118\n",
      "Validation Loss decreased ( 1.8126 --->  1.8118. Saving Model)\n",
      "Epoch : 57 \tTraining Loss :  1.3098 \tValidation Loss :  1.8273\n",
      "Epoch : 58 \tTraining Loss :  1.2850 \tValidation Loss :  1.8139\n",
      "Epoch : 59 \tTraining Loss :  1.2828 \tValidation Loss :  1.8481\n",
      "Epoch : 60 \tTraining Loss :  1.2622 \tValidation Loss :  1.8392\n",
      "Epoch : 61 \tTraining Loss :  1.2477 \tValidation Loss :  1.8457\n",
      "Epoch : 62 \tTraining Loss :  1.2364 \tValidation Loss :  1.8413\n",
      "Epoch : 63 \tTraining Loss :  1.2218 \tValidation Loss :  1.7894\n",
      "Validation Loss decreased ( 1.8118 --->  1.7894. Saving Model)\n",
      "Epoch : 64 \tTraining Loss :  1.2176 \tValidation Loss :  1.8230\n",
      "Epoch : 65 \tTraining Loss :  1.1929 \tValidation Loss :  1.8039\n",
      "Epoch : 66 \tTraining Loss :  1.1810 \tValidation Loss :  1.7912\n",
      "Epoch : 67 \tTraining Loss :  1.1615 \tValidation Loss :  1.7886\n",
      "Validation Loss decreased ( 1.7894 --->  1.7886. Saving Model)\n",
      "Epoch : 68 \tTraining Loss :  1.1519 \tValidation Loss :  1.8353\n",
      "Epoch : 69 \tTraining Loss :  1.1469 \tValidation Loss :  1.7872\n",
      "Validation Loss decreased ( 1.7886 --->  1.7872. Saving Model)\n",
      "Epoch : 70 \tTraining Loss :  1.1256 \tValidation Loss :  1.8252\n",
      "Epoch : 71 \tTraining Loss :  1.1106 \tValidation Loss :  1.8096\n",
      "Epoch : 72 \tTraining Loss :  1.0864 \tValidation Loss :  1.8471\n",
      "Epoch : 73 \tTraining Loss :  1.0984 \tValidation Loss :  1.8191\n",
      "Epoch : 74 \tTraining Loss :  1.0784 \tValidation Loss :  1.8263\n",
      "Epoch : 75 \tTraining Loss :  1.0554 \tValidation Loss :  1.8255\n",
      "Stop Execution : Mon Sep 16 06:38:20 2019\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print (\"Start Execution : \",end=\"\") \n",
    "start_time = time.ctime()\n",
    "print(start_time) \n",
    "epochs = 75\n",
    "val_loss_min = np.Inf\n",
    "for e in range(epochs):\n",
    "    training_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    network.train()\n",
    "    for images,labels in train_loader:\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            images,labels = images.cuda(),labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = network.forward(images)\n",
    "        loss = criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()*images.size(0)\n",
    "    \n",
    "    network.eval()\n",
    "    for images,labels in val_loader:\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            images,labels = images.cuda(),labels.cuda()\n",
    "        \n",
    "        output = network.forward(images)\n",
    "        loss = criterion(output,labels)\n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "        \n",
    "    training_loss = training_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(val_loader.sampler)\n",
    "    \n",
    "    print('Epoch : {} \\tTraining Loss : {: .4f} \\tValidation Loss : {: .4f}'.format(e+1,training_loss,valid_loss))\n",
    "    \n",
    "    if valid_loss <= val_loss_min:\n",
    "        print('Validation Loss decreased ({: .4f} ---> {: .4f}. Saving Model)'.format(val_loss_min,valid_loss))\n",
    "        torch.save(network.state_dict(),'model_detail.pt')\n",
    "        val_loss_min = valid_loss\n",
    "print (\"Stop Execution : \",end=\"\") \n",
    "end_time = time.ctime()\n",
    "print(end_time) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.load_state_dict(torch.load('model_detail.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Execution : Mon Sep 16 06:38:22 2019\n",
      "Test Loss: 1.7064\n",
      "\n",
      "Test Accuracy of apple: 76% (76/100)\n",
      "Test Accuracy of aquarium_fish: 63% (63/100)\n",
      "Test Accuracy of  baby: 49% (49/100)\n",
      "Test Accuracy of  bear: 37% (37/100)\n",
      "Test Accuracy of beaver: 28% (28/100)\n",
      "Test Accuracy of   bed: 51% (51/100)\n",
      "Test Accuracy of   bee: 60% (60/100)\n",
      "Test Accuracy of beetle: 57% (57/100)\n",
      "Test Accuracy of bicycle: 74% (74/100)\n",
      "Test Accuracy of bottle: 69% (69/100)\n",
      "Test Accuracy of  bowl: 52% (52/100)\n",
      "Test Accuracy of   boy: 42% (42/100)\n",
      "Test Accuracy of bridge: 57% (57/100)\n",
      "Test Accuracy of   bus: 39% (39/100)\n",
      "Test Accuracy of butterfly: 53% (53/100)\n",
      "Test Accuracy of camel: 45% (45/100)\n",
      "Test Accuracy of   can: 60% (60/100)\n",
      "Test Accuracy of castle: 74% (74/100)\n",
      "Test Accuracy of caterpillar: 47% (47/100)\n",
      "Test Accuracy of cattle: 40% (40/100)\n",
      "Test Accuracy of chair: 73% (73/100)\n",
      "Test Accuracy of chimpanzee: 81% (81/100)\n",
      "Test Accuracy of clock: 50% (50/100)\n",
      "Test Accuracy of cloud: 74% (74/100)\n",
      "Test Accuracy of cockroach: 74% (74/100)\n",
      "Test Accuracy of couch: 35% (35/100)\n",
      "Test Accuracy of  crab: 50% (50/100)\n",
      "Test Accuracy of crocodile: 45% (45/100)\n",
      "Test Accuracy of   cup: 67% (67/100)\n",
      "Test Accuracy of dinosaur: 58% (58/100)\n",
      "Test Accuracy of dolphin: 61% (61/100)\n",
      "Test Accuracy of elephant: 52% (52/100)\n",
      "Test Accuracy of flatfish: 40% (40/100)\n",
      "Test Accuracy of forest: 56% (56/100)\n",
      "Test Accuracy of   fox: 59% (59/100)\n",
      "Test Accuracy of  girl: 45% (45/100)\n",
      "Test Accuracy of hamster: 45% (45/100)\n",
      "Test Accuracy of house: 44% (44/100)\n",
      "Test Accuracy of kangaroo: 53% (53/100)\n",
      "Test Accuracy of keyboard: 64% (64/100)\n",
      "Test Accuracy of  lamp: 47% (47/100)\n",
      "Test Accuracy of lawn_mower: 76% (76/100)\n",
      "Test Accuracy of leopard: 60% (60/100)\n",
      "Test Accuracy of  lion: 56% (56/100)\n",
      "Test Accuracy of lizard: 33% (33/100)\n",
      "Test Accuracy of lobster: 45% (45/100)\n",
      "Test Accuracy of   man: 36% (36/100)\n",
      "Test Accuracy of maple_tree: 57% (57/100)\n",
      "Test Accuracy of motorcycle: 75% (75/100)\n",
      "Test Accuracy of mountain: 68% (68/100)\n",
      "Test Accuracy of mouse: 23% (23/100)\n",
      "Test Accuracy of mushroom: 53% (53/100)\n",
      "Test Accuracy of oak_tree: 74% (74/100)\n",
      "Test Accuracy of orange: 81% (81/100)\n",
      "Test Accuracy of orchid: 77% (77/100)\n",
      "Test Accuracy of otter: 12% (12/100)\n",
      "Test Accuracy of palm_tree: 77% (77/100)\n",
      "Test Accuracy of  pear: 57% (57/100)\n",
      "Test Accuracy of pickup_truck: 68% (68/100)\n",
      "Test Accuracy of pine_tree: 45% (45/100)\n",
      "Test Accuracy of plain: 82% (82/100)\n",
      "Test Accuracy of plate: 50% (50/100)\n",
      "Test Accuracy of poppy: 54% (54/100)\n",
      "Test Accuracy of porcupine: 50% (50/100)\n",
      "Test Accuracy of possum: 19% (19/100)\n",
      "Test Accuracy of rabbit: 29% (29/100)\n",
      "Test Accuracy of raccoon: 59% (59/100)\n",
      "Test Accuracy of   ray: 32% (32/100)\n",
      "Test Accuracy of  road: 79% (79/100)\n",
      "Test Accuracy of rocket: 68% (68/100)\n",
      "Test Accuracy of  rose: 54% (54/100)\n",
      "Test Accuracy of   sea: 73% (73/100)\n",
      "Test Accuracy of  seal: 33% (33/100)\n",
      "Test Accuracy of shark: 48% (48/100)\n",
      "Test Accuracy of shrew: 41% (41/100)\n",
      "Test Accuracy of skunk: 81% (81/100)\n",
      "Test Accuracy of skyscraper: 77% (77/100)\n",
      "Test Accuracy of snail: 40% (40/100)\n",
      "Test Accuracy of snake: 38% (38/100)\n",
      "Test Accuracy of spider: 56% (56/100)\n",
      "Test Accuracy of squirrel: 26% (26/100)\n",
      "Test Accuracy of streetcar: 58% (58/100)\n",
      "Test Accuracy of sunflower: 81% (81/100)\n",
      "Test Accuracy of sweet_pepper: 59% (59/100)\n",
      "Test Accuracy of table: 36% (36/100)\n",
      "Test Accuracy of  tank: 71% (71/100)\n",
      "Test Accuracy of telephone: 57% (57/100)\n",
      "Test Accuracy of television: 64% (64/100)\n",
      "Test Accuracy of tiger: 65% (65/100)\n",
      "Test Accuracy of tractor: 69% (69/100)\n",
      "Test Accuracy of train: 62% (62/100)\n",
      "Test Accuracy of trout: 65% (65/100)\n",
      "Test Accuracy of tulip: 49% (49/100)\n",
      "Test Accuracy of turtle: 35% (35/100)\n",
      "Test Accuracy of wardrobe: 86% (86/100)\n",
      "Test Accuracy of whale: 55% (55/100)\n",
      "Test Accuracy of willow_tree: 53% (53/100)\n",
      "Test Accuracy of  wolf: 62% (62/100)\n",
      "Test Accuracy of woman: 22% (22/100)\n",
      "Test Accuracy of  worm: 58% (58/100)\n",
      "\n",
      "Test Accuracy (Overall): 55% (5515/10000)\n",
      "Stop Execution : Mon Sep 16 06:38:26 2019\n"
     ]
    }
   ],
   "source": [
    "print (\"Start Execution : \",end=\"\") \n",
    "start_time_test = time.ctime()\n",
    "print(start_time_test) \n",
    "test_loss = 0.0\n",
    "batch_size = 20\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "network.eval()\n",
    "for images,labels in test_loader:\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        images,labels = images.cuda(),labels.cuda()\n",
    "    output = network.forward(images)\n",
    "    loss = criterion(output,labels)\n",
    "    test_loss += loss.item()*images.size(0)\n",
    "    _,pred = torch.max(output, 1)\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        target = labels.data[i]\n",
    "        class_correct[target] += correct[i].item()\n",
    "        class_total[target] += 1\n",
    "        \n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.4f}\\n'.format(test_loss))\n",
    "for i in range(100):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "print (\"Stop Execution : \",end=\"\") \n",
    "end_time_test = time.ctime()\n",
    "print(end_time_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
